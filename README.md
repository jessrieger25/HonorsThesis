# Guide to Ficino Text Analysis

This project serves as  collection of tools that aid in processing Ficino's works and 
understanding the semantic meaning behind them without the influence of human perception
and biases.

### Introduction

Within the project there are two main sections, word occurrence description and sentiment analysis
models. The former is based on a list of categorized keywords generated by James Snyder and his peers. The three different
tasks run through the text word by word, and either note the occurrence of other keywords around a specified keyword, 
calculate the average distance of each other keyword from a particular word, or print all the occurrences of a word with
a window of words of a specified size surrounding each occurrence. 

The average distance of words and the occurrence of keywords within a particular window can then be visualized in graph 
form using G* Studio. 

The analysis based on trained sentiment models come in 3 different forms, namely the 
Skip Gram model, the GloVe model, and the LSTM model trained using the Watson Sentiment model. All of the models work to
create an embedded matrix that represents and understanding of the words in our 'vocabulary', which can then be 
visualized to show the semantic relations between words in the text. 

The embedding matrix can then be reduced in its dimensionality by the TSNE algorithm and plotted on a 
scatterplot using Python's Matplotlib library.

### Project Directory Structure

```
Honors_Thesis_2
 |__ ficino - Ficino Text Files
 |__ graphics_ficino - Output graphics of analysis on the Ficino Text
 |__ models - Directory containing all code for sentiment analysis models
        |__ glove_text - Documents containing word representations from the GloVe Project
        |__ watson_api - Code to get the labels for the Ficino data from the Watson API
        |__ analysis_drivers.py - Main file to select which model to run and drive the run
        |__ bag_of_words.py - Basic code to make bag of words from text
        |__ glove.py - Glove Model Analysis
        |__ lstm_keras.py - LSTM Model Analysis using Keras Library
        |__ skip_gram_tutorial.py - Skip Gram Model Analysis
        |__ tsne.py - TSNE visualization code
        |__ word_prep.py - Vocabulary creation, Word to Int creation, tokenization of sentences and words
 |__ text_parsing - Word occurrence analysis
        |__ g_star_graphs - G* graph scripts
        |__ word_lists - Keywords list and list of words to ignore
        |__ driver.py - Main program to choose which analysis to run
        |__ positional_relations.py - Code for each type of analysis
        |__ creating_graph.py - Creates G* graph script
 |__ README.md - Guide to using tools
```

### Usage

##### Setup

This tutorial is meant to be used with Python 3 and pip3. You could adjust it to use Python 2 as well, but please don't.

Download and install Python 3.6 from https://www.python.org/ .
It will likely include a back level version of pip3. To upgrade to the latest version of pip, 
run: `python -m pip install --upgrade pip`

*Python Requirements:*

To install all python requirements, run: `pip3 install -r requirements.txt`

Then set the Python Path environment variable to the main directory of the project. E.g.,  
`export PYTHONPATH = /path/to/HonorsThesis/folder`

in Unix. Or, in Windows:

`set PythonPath=D:\Research\JR-HonorsThesis-master`

*GloVe Requirements:*

This project requires the pretrained vectors from the Stanford GloVe project.

To install: 
- Visit: https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation
- Download all files
- Create a folder called `glove_text` in the `models` folder
- Place the files in the newly created folder: `models/glove_text`

*NLTK Requirements:*

You'll need to download some NLTK resources, so fire up IDLE, the default Python IDE.

Enter the following commands:
```
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```
Then exit IDLE.

*Other Dependencies:*

- imagemagick
- ffmpeg

On Mac, these can be installed using `brew install <name>`.
On Windows, download imagemagick from `https://imagemagick.org/script/download.php#windows`. The install should give you an option to install ffmpeg as well.

If, after all of this, you are still seeing errors based on packages, you may need to pip3 install other dependencies.

 
##### Model Analysis

*Setup:* 

- Select the analysis_drivers.py file.
- Make sure at the bottom, the correct file list is entered as a parameter for the script. This will be the text
you want to analyze.
- Make sure the keywords are specified in keywords.txt and the categories are separated by a blank line.

*Run:*

- Run analysis_drivers.py: `python3 models/analysis_drivers.py <selected option>`
- The <selected option> should be one of the following: 
    * s for skip gram
    * g for glove
    * w for watson (DO NOT USE)
- Wait for the model to complete. The runtime varies significanly. Some scripts make take upwards of 24 hours to run.

*Interpretation:*

Bag of Words:

This will create a bag of words representation of the text using the Count Vectorizer from sklearn.
This representation is then fed into the LatentDirichletAllocation function from sklearn which recognizes relations in the
representation and prints out categories that are found in the text.

You will see a list of topics/categories with a few of their word members.

Skip Gram Model and GloVe:

This will display a graph created by generating an embedding matrix, that represents the semantic relations of words, and is
then graphed using TSNE. 

**NOTE: only the keywords are graphed as a graph of all words would be dominated by non-categorized words.**

In the skip gram model, the embedding matrix probabilites are generated based on training with the actual text, whereas
the embedding matrix of GloVe uses their pre-known vector representations of the words in the text. 

##### Word Occurrence Analysis

*Setup:*

- Select the positional_relations.py file
- Make sure the source_files list contains the paths of the text files to be analyzed
- Make sure the appropriate words are specified/categorized in keywords.txt and ignored.txt

*Run:*

- Run the driver.py file: `python3 test_parsing/driver.py`
- Enter the command for the analysis type to run:
    * wr: within range
    * a: average distance
    * p: print surrounding words
- Look at the terminal for printed output, or the g_star_graph files for output

*Interpretation:*

Within Range: 

Finds the keywords that are within a certain range of a specified keyword. The graph generated shows the center node as
the main keyword specified, and the connected nodes are those that appear within the specified range of the main keyword. 

Average Distance:

Finds the average distance each keyword is from the specified keyword. The graph generated shows a node for each keyword
where the size represents the average distance from the specified keyword.

Print Surrounding Words:

Prints all occurrences of a specified keyword with a specified window of words that surround it.

**Note: Within range and average distance both generate the script for a G Star graph that is located in the g_star_graphs folder.**


